I"º<p>Suppose we are doing Gaussian Mixture (1D). The histogram of posterior distribution is (we choose a new <script type="math/tex">z_i</script> from this histogram),</p>

<p><span style="font-size:0.8em; line-height:0%">
<script type="math/tex">% <![CDATA[
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\begin{align}
&\qquad \int p(x_i | z_i=k, \mu_k, \sigma^2) p(\mu_k | \bx^{\backslash i}, \bz^{\backslash i}, \mu_P, \sigma^2_P, \sigma^2) d\mu_k \int p(z_i =k|\btheta) p(\btheta | \bz^{\backslash i},\balpha) d\btheta \\
&= \cN(x_i | \mu_{\rm New}, \sigma^2) \cdot \frac{n_k^{\backslash i} + \alpha_k}{\sum_{k=1}^{K} n_k^{\backslash i} + \alpha_k}
\end{align} %]]></script>
</span></p>

<p>Each integral shows posterior predictive distribution of <script type="math/tex">x_i</script> and <script type="math/tex">z_i</script>, respectively. We can consider the expectation of <script type="math/tex">\mu_k</script> for the first term instead of calculating everything. <script type="math/tex">\mu_k</script> can take various values, but it follows Normal distribution. Enough amount of data makes the posterior distribution of <script type="math/tex">\mu_k</script> sharp. The expectation can be a good approximation.</p>

<p><span style="font-size:0.8em; line-height:0%">
<script type="math/tex">% <![CDATA[
\newcommand{\balpha}{\boldsymbol{\alpha}}
\newcommand{\bz}{\mathbf{z}}
\newcommand{\cN}{\mathcal{N}}
\newcommand{\bx}{\mathbf{x}}
\newcommand{\btheta}{\boldsymbol{\theta}}
\begin{align}
  &\qquad p(x_i | z_i=k, {\mu_k}, \sigma^2) \int  p(\mu_k | \bx^{\backslash i}, \bz^{\backslash i}, \mu_P, \sigma^2_P, \sigma^2) d\mu_k\\
  &= p(x_i | z_i=k, \overline{\mu_k}, \sigma^2)
\end{align} %]]></script>
</span></p>

<p>Code can be found <a href="https://github.com/Shusei-E/Code_Tips/tree/master/Algorithm/MCMC/Gibbs_Sampling/GMM1D">here</a>.</p>
:ET